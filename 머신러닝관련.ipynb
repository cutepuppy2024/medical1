{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.데이터 확인\n",
    "# nan, 0, type, 데이터간격\n",
    "# str에 대한 데이터 분리\n",
    "# describe, info()\n",
    "# 결과값의 파악 : picher['연봉(2018)']\n",
    "# hist그래프\n",
    "# 각각 피쳐간 상관관계 corr, heatmap\n",
    "# - 피쳐간 관계 : 비슷한 피쳐는 삭제\n",
    "# 각각 피쳐를 출력 subplots 각각 데이터 분포파악\n",
    "# (0 데이터가 어느정도 있는지, 데이터 분포가 어떻게 되는지)\n",
    "\n",
    "# 1.데이터 전처리 \n",
    "# - data, result 분리\n",
    "# - train, test 분리\n",
    "# - 표준화 작업\n",
    "# 2.데이터 학습시키기\n",
    "# 3.데이터 정답률\n",
    "\n",
    "# -------------------------------\n",
    "# 4.다시 학습시키기\n",
    "# 5. 경사하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# < rcParams >\n",
    "\n",
    "Artists = set()\n",
    "for key in matplotlib.rcParams.keys():\n",
    "    if \".\" in key:\n",
    "        Artists .add(key.split(\".\")[0])\n",
    "\n",
    "print(Artists)\n",
    "{'_internal',\n",
    " 'agg', 'animation','axes',\n",
    " 'axes3d','boxplot','contour',\n",
    " 'date','docstring','errorbar',\n",
    " 'figure','font','grid',\n",
    " 'hatch','hist', 'image',\n",
    " 'keymap','legend','lines',\n",
    " 'markers','mathtext','patch',\n",
    " 'path','pcolor', 'pcolormesh',\n",
    " 'pdf','pgf','polaraxes',\n",
    " 'ps','savefig', 'scatter',\n",
    " 'svg','text', 'tk',\n",
    " 'webagg', 'xaxis','xtick',\n",
    " 'yaxis', 'ytick'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# < metrics : 모델의 성능을 지표로 표현하는 것 >\n",
    "url = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb5JEm3%2FbtrDkU9SO2t%2FxGK7kFE83jAm7cKkpxiglk%2Fimg.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# < numpy >\n",
    "df.to_numpy()\n",
    "\n",
    "\n",
    "np.zeros(), np.ones(), np.NaN, np.full((),)\n",
    "\n",
    "np.array()\n",
    "np.arange() , a.reshape(())\n",
    "np.unique()\n",
    "\n",
    "np.add(,), np.multiply(,), np.divide(,), np.substract(,)\n",
    "dot(,) # 행열의 곱\n",
    "sum(), prod(), mean(), std() / axis = 0(컬럼) 1(행)\n",
    "'%.2f%np.std()'\n",
    "\n",
    "# 선형대수함수\n",
    "a.T\n",
    "# 난수생성\n",
    "np.random.seed() # 유사난수생성\n",
    "np.random.uniform() \n",
    "np.random.normal() # 정규분포\n",
    "np.random.shuffle()\n",
    "\n",
    "np.column_skack() # 컬럼화\n",
    "np.concatenate() # 배열합치기\n",
    "\n",
    "# numpy로 랜덤데이터 분리시키기\n",
    "np.random.seed(0)\n",
    "idx = np.arange()\n",
    "np.random.shuffle(idx)\n",
    "df.iloc[idx[:],:]\n",
    "# <-> 파이썬으로 랜덤데이터 분리시키기\n",
    "idx = [i for i in range()]\n",
    "random.shuffle(idx)\n",
    "df.iloc[idx[:],:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "# 전처리\n",
    "from sklearn.preprocessing import PolynomialFeatures  # 선형회귀 - 자동으로 특성 제곱을 자동으로 만들어주는 라이브러리 => 특성의 값을 늘려줌\n",
    "from sklearn.preprocessing import StandardScaler  # 표준화\n",
    "\n",
    "# model_selection 패키지\n",
    "from sklearn.model_selection import train_test_split # 데이터를 train, test로 분리하는 함수\n",
    "\n",
    "# neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# linear_model 패키지\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 전처리\n",
    "\n",
    "datas = [[l,w,r] for l,w,r in zip(length,weight,results)] \n",
    "distances, indexs = clf.kneighbors([[25,150]])\n",
    "\n",
    "\n",
    "# 원핫인코딩\n",
    "target = [] \n",
    "data = []\n",
    "for idx, row in df.iterrows():\n",
    "    target.append(row.loc[0])\n",
    "    row_data = []\n",
    "    for v in row.loc[1:]:\n",
    "        row_data.append(ord(v))\n",
    "    data.append(row_data)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    for j in range(len(df.columns)):\n",
    "        l_pos = df.iloc[i,j]\n",
    "        ord(l_pos)\n",
    "        df.iloc[i,j] = ord(l_pos)\n",
    "\n",
    "pd.get_dummies(df['Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, target\n",
    "# train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준점수 = (데이터값 - mean) / std : 각 특성값이 평균에서 표준편차의 몇배수 떨어져 있는지  // 분산 = (데이터값 - mean)**2 -> 평균   # 표준편차(std) = 분산의 제곱근(데이터가 분산된 정도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 학습시키기\n",
    "# 모듈선언 -> fit()\n",
    "\n",
    "## < 분류 >\n",
    "clf = svm.SVC()\n",
    "# 1) Support Vector Machine : 데이터 간의 간격이 최대가 되는 선을 찾아 기준으로 만들고 분류\n",
    "\n",
    "# 2) knn\n",
    "knn = KNeighborsClassifier()\n",
    "# K-Nearest Neighbors : 근접한 이웃의 개수를 비교하여 판단\n",
    "# -> 표준점수를 구해 적용 : 그래프의 간격을 맞추어야 함\n",
    "#    train_scaled = (train_input - mean) / std\n",
    "# -> n_neighbors : 이웃의 개수\n",
    "# p : 거리를 재는 방법 ( 1 -  맨해튼거리, 2 - 유클리디안거리(default))\n",
    "# n_jobs :  매개변수로 사용할 CPU 지정(-1 : 모든 CPU코어 사용/ 이웃간의 거리 계산 속도를 높일 수 있음)\n",
    "\n",
    "\n",
    "# 3) 로지스틱 회귀\n",
    "lr = LogisticRegression()\n",
    "predict_proba()\n",
    "np.round(proba,decimals=4)\n",
    "z = np.arange(-5,5,0.1)  # -5부터 5까지 0.1단위로 배열생성\n",
    "# 시그모이드 함수\n",
    "phi = 1/(1 + np.exp(-z))\n",
    "# 그래프 출력 - 100개 직선\n",
    "plt.plot(z,phi)  \n",
    "# \n",
    "decision_function()\n",
    "# 시그모이드 함수를 적용한 값\n",
    "expit(decisions)\n",
    "# 시그모이드 함수, softmax(전체합이 1-> 확률로 만듬)\n",
    "\n",
    "\n",
    "# ->  다시 학습\n",
    "lr = LogisticRegression(C=20,max_iter=1000) \n",
    "\n",
    "# 4) 경사하강법\n",
    "sc = SGDClassifier(loss='log_loss',max_iter=10,random_state=42)\n",
    "cross_validate()\n",
    "# -> partial_fit\n",
    "# 데이터 일부를 가져와서 훈련을 해서 target 맞추는데, 일부만 가져와서 전체적인 target을 알지 못함\n",
    "classes = np.unique(train_target) # 전체적인 target이 무엇인지 알려줘야 함 # 꼭 classes를 추가\n",
    "sc.partial_fit(train_scaled, train_target, classes=classes)\n",
    "# -> partial_fit 반복\n",
    "train_score = []\n",
    "test_score = []\n",
    "for _ in range(0,300):\n",
    "    sc.partial_fit(train_scaled, train_target, classes=classes)\n",
    "    train_score.append(sc.score(train_scaled, train_target))\n",
    "    test_score.append(sc.score(test_scaled, test_target))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# 4) 랜덤포레스트\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "rfc.feature_importances_ # 특성에 대한 중요도\n",
    "    \n",
    "# --------------------------------------------------------------------------------------------------\n",
    "## < 예측 > : 과대/과소적합\n",
    "\n",
    "# 1) knr = KNeighborsRegressor()\n",
    "# 결정계수 R**2 (0~1의 값)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error() # 평균 절대값 오차 \n",
    "test_mean = mean_absolute_error(test_target, predict)\n",
    "# -> 과소적합인 경우 n_neighbors의 수를 줄여서 훈련 : knr.n_neighbors = 3\n",
    "# knn회귀의 단점은 훈련세트가 없는 것은 데이터 예측 제대로 안된다는 것\n",
    "\n",
    "\n",
    "# 2) 선형회귀\n",
    "lr = LinearRegression()\n",
    "# 선형회귀 : fit을 하면 기울기와 y절편이 구해짐\n",
    "print(lr.coef_, lr.intercept_)  # lr.coef  :  기울기, lr.intercept_ :  y절편 # _  : 모델을 훈련시켜서 모델에서 제공하는 변수는 _ 사용\n",
    "# plt.plot([15,50],[15*lr.coef_+lr.intercept_,50*lr.coef_+lr.intercept_ ])\n",
    "\n",
    "# ->  다항회귀 : 특성을 제곱시켜줌 (마이너스 값이 나오기 때문)\n",
    "poly.get_feature_names_out() : 특성 확인\n",
    "# train_poly = np.column_stack((train_input**2, train_input))\n",
    "# test_poly = np.column_stack((test_input**2,test_input))\n",
    "# x = np.arange(15,50)\n",
    "# plt.plot(x,(1.014*x)**2 +(-21.55)*x+ 116.1)  # 기울기*x**2+ 기울기*x + y절편\n",
    "\n",
    "poly =  PolynomialFeatures(degree = 3, include_bias=False) # 특성을 늘려줌\n",
    "poly.fit(train_input)\n",
    "train_poly = poly.transform(train_input) \n",
    "test_poly = poly.transform(test_input)\n",
    "\n",
    "# -> 늘어난 특성값에 대한 표준화 적용\n",
    "ss = StandardScaler() # 표준화적용 : 선형회귀에서 특성의 값이 많이 늘어났을 때, 특성의 값을 규제, test세트에서도 제대로 예측할 수 있도록 함\n",
    "ss.fit(train_poly) \n",
    "train_scaled =  ss.transform(train_poly)\n",
    "test_scaled =  ss.transform(test_poly)\n",
    "# -> 규제\n",
    "ridge =  Ridge() # 규제 \n",
    "# -> 규제 후 알파값 적용\n",
    "ridge =  Ridge(alpha=) # train - test 간격이 가장 가까운 구간을 찾음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict()\n",
    "score() / metrics.accuracy_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식\n",
    "# . : 하나의 문자\n",
    "# ^ : 첫문자열의 시작\n",
    "# $ : 문자열의 끝\n",
    "# [ ] : 대괄호 안에 글자가 일치하려는 문자를 입력시킴\n",
    "    # [0-9] : 0부터 9까지의 숫자가 일치하는지 확인\n",
    "    # [a-zA-Z] : 영문자와 일치하는지 확인\n",
    "    # [ㄱ-ㅎ가-힣] : 국문자와 일치하는지 확인\n",
    "# {} : 숫자의 길이\n",
    "    # {2} : 문자의 길이가 2자리\n",
    "    # {3,} : 문자의 길이가 3자리 이상\n",
    "    # {,3} : 문자의 길이가 3자리 까지\n",
    "    # {2,3} : 문자의 길이가 2자리에서 3자리까지 확인\n",
    "\n",
    "# match() 처음부터 모두 일치하는 것인지 확인\n",
    "# search()  일치하는 것이 있는지 확인\n",
    "# findall() 일치하는 것이 있는지 리스트로부터 확인\n",
    "# group() 일치하면 해당되는 문자를 출력\n",
    "\n",
    "# compile() 정규식 형태를 지정\n",
    "# sub()  일치하는 데이터를 삭제\n",
    "\n",
    "import re\n",
    "p = re.compile(\"ca.e\") # 정규표현식을 지정\n",
    "p.match/search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig,axs = plt.subplots(4,2,figsize=(10,14))\n",
    "sns.distplot(df['컬럼명'],ax=axs[0,0]) # x축만으로 그래프를 그림 / \n",
    "\n",
    "df.corr()\n",
    "sns.heatmap(df.corr(),annot=True)\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "# 상관도에 대한 그래프\n",
    "sns.pairplot(df,hue='Outcome') # 3차원 이상일 때, dataframe의 각 열을 인수로 조합 / hue : 카테고리 변수 이름을 지정하여 카테고리 값에 따라 색상을 다르게 할 수 있음 ex) markers=[\"o\", \"s\", \"D\"]\n",
    "\n",
    "sns.rugplot(x) # 데이터 위치를 x축 위에 작은 선분(rug)으로 나타내어 실제 데이터들의 위치를 보여줌\n",
    "sns.kdeplot(x) # 커널밀도추정 \n",
    "sns.distplot() # 러그와 커널 밀도 2가지 모두 표시\n",
    "sns.countplot(x=\"column_name\", data=dataframe)  # 명령을 사용하면 각 카테고리 값별로 데이터가 얼마나 있는지 표시, dataframe에만 사용\n",
    "sns.jointplot(x=\"sepal_length\", y=\"sepal_width\", data=iris,kind=\"kde\"/'scatter' ) # 인수에는 대상이 되는 데이터프레임을, x 인수에는 x 변수가 될 데이터프레임의 열 이름 문자열을, y 인수에는 y 변수가 될 데이터프레임의 열 이름 문자열\n",
    "sns.FacetGrid()\n",
    "\n",
    "plot.bar()\n",
    "ax = sns.countplot(x='pclass',hue='survived', data = df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(df, index = ['계정코드','계정과목']) # 그루핑을 2차원으로 하는 함수"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
